{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luv4as/mdpi_ia4good/blob/master/GrafIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GrafIA"
      ],
      "metadata": {
        "id": "y-iZbB6usTNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instala√ß√£o e Imports"
      ],
      "metadata": {
        "id": "FAllwbNQscim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala a NOVA biblioteca do Google (v2) e Pydantic\n",
        "!pip install -q google-genai pydantic transformers accelerate bitsandbytes neo4j sentencepiece gradio\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from neo4j import GraphDatabase\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas e bibliotecas importadas!\")"
      ],
      "metadata": {
        "id": "h01NXsuMsr3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccb9f12-e495-4f43-9680-08a286bbf8af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Depend√™ncias instaladas e bibliotecas importadas!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configura√ß√£o e Conex√µes"
      ],
      "metadata": {
        "id": "LYw7Ob1ossYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Carrega as chaves\n",
        "    NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "    NEO4J_USER = userdata.get('NEO4J_USERNAME').strip() # Added .strip()\n",
        "    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD').strip() # Added .strip()\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "    driver.verify_connectivity()\n",
        "    print(\"‚úÖ Conex√£o Neo4j estabelecida com sucesso!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO DE CONEX√ÉO: {e}\")\n",
        "    print(\"Verifique se voc√™ preencheu os 'Secrets' (chavinha na esquerda) corretamente.\")"
      ],
      "metadata": {
        "id": "iJNJjvXhsva5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22479ce-72c2-4daa-f06f-c5c25d90a596"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conex√£o Neo4j estabelecida com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Carregar Modelo Local (LiquidAI)"
      ],
      "metadata": {
        "id": "HczC9Oxdszf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚è≥ Carregando modelo LiquidAI (LFM2-1.2B) na GPU...\")\n",
        "\n",
        "model_id = \"LiquidAI/LFM2-1.2B-RAG\" # Modelo otimizado\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16, # Importante para caber na T4\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado na mem√≥ria GPU!\")"
      ],
      "metadata": {
        "id": "yl-TpBgFtAq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2abe88-ac89-4ade-afa0-eb2f457055c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Carregando modelo LiquidAI (LFM2-1.2B) na GPU...\n",
            "‚úÖ Modelo carregado na mem√≥ria GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Fun√ß√µes do Pipeline (L√≥gica)"
      ],
      "metadata": {
        "id": "31UJncIshhhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Schemas"
      ],
      "metadata": {
        "id": "cffiQSW7pHIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(BaseModel):\n",
        "    id: str = Field(description=\"Nome √∫nico do conceito ou entidade\")\n",
        "    type: str = Field(description=\"Tipo do n√≥ (ex: Concept, Tool, Method)\", default=\"Concept\")\n",
        "\n",
        "class Edge(BaseModel):\n",
        "    source: str = Field(description=\"N√≥ de origem\")\n",
        "    target: str = Field(description=\"N√≥ de destino\")\n",
        "    relation: str = Field(description=\"Rela√ß√£o em CAIXA_ALTA (ex: RELATES_TO)\")\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    nodes: List[Node]\n",
        "    edges: List[Edge]"
      ],
      "metadata": {
        "id": "H-XYRnO-ou4n"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Extra√ß√£o"
      ],
      "metadata": {
        "id": "oUh6ISJ5hpXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step1_extrair_termos(texto):\n",
        "    \"\"\"Extrai APENAS as entidades principais (sujeitos)\"\"\"\n",
        "    print(f\"1. [LiquidAI] Analisando: {texto[:50]}...\")\n",
        "\n",
        "    if 'tokenizer' not in globals() or 'model' not in globals():\n",
        "        return [texto]\n",
        "\n",
        "    # Prompt refor√ßado para pegar KEYWORDS e n√£o frases\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Extract only the main technical KEYWORDS from the text. Ignore verbs and questions. Output JSON List.\"},\n",
        "        {\"role\": \"user\", \"content\": 'Input: \"O que √© RAG?\"'},\n",
        "        {\"role\": \"assistant\", \"content\": '[\"RAG\"]'},\n",
        "        {\"role\": \"user\", \"content\": 'Input: \"Explique como funciona a Fotoss√≠ntese\"'},\n",
        "        {\"role\": \"assistant\", \"content\": '[\"Fotoss√≠ntese\"]'},\n",
        "        {\"role\": \"user\", \"content\": f'Input: \"{texto}\"'}\n",
        "    ]\n",
        "\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs, max_new_tokens=64, do_sample=False, temperature=0.1,\n",
        "        pad_token_id=tokenizer.eos_token_id, attention_mask=torch.ones_like(inputs)\n",
        "    )\n",
        "\n",
        "    raw = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Fun√ß√£o de limpeza recursiva (Mantida pois √© boa)\n",
        "    def extrair_strings(dados):\n",
        "        lista = []\n",
        "        if isinstance(dados, str): return [dados]\n",
        "        if isinstance(dados, list):\n",
        "            for item in dados: lista.extend(extrair_strings(item))\n",
        "        if isinstance(dados, dict):\n",
        "            for v in dados.values(): lista.extend(extrair_strings(v))\n",
        "        return lista\n",
        "\n",
        "    try:\n",
        "        clean = raw.replace(\"```json\", \"\").replace(\"```\", \"\").split(\"Output:\")[-1].strip()\n",
        "        dados_brutos = json.loads(clean)\n",
        "        termos_limpos = extrair_strings(dados_brutos)\n",
        "\n",
        "        # Filtros extras: Remove palavras comuns que atrapalham a busca\n",
        "        stopwords = [\"o\", \"que\", \"√©\", \"de\", \"para\", \"como\", \"explique\", \"conceito\", \"fundamental\"]\n",
        "        termos_finais = list(set([t for t in termos_limpos if len(t) > 2 and t.lower() not in stopwords]))\n",
        "\n",
        "        print(f\"   -> Termos Finais: {termos_finais}\")\n",
        "        return termos_finais if termos_finais else [texto]\n",
        "    except:\n",
        "        return [texto]"
      ],
      "metadata": {
        "id": "iTUllwT6hrQR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Expans√£o"
      ],
      "metadata": {
        "id": "E25QhUAfhv6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step2_gerar_grafo(termos, texto_original):\n",
        "    \"\"\"Usa Gemini 2.5 Lite (10 RPM) para economizar cota\"\"\"\n",
        "    print(f\"2. [Gemini 2.5 Lite] Criando estrutura estruturada...\")\n",
        "\n",
        "    # Modelo mais leve da sua lista\n",
        "    MODELO_ESTRUTURA = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "    try:\n",
        "        client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Voc√™ √© um arquiteto de Knowledge Graphs.\n",
        "        Contexto do Usu√°rio: \"{texto_original}\"\n",
        "        Entidades Chave: {termos}\n",
        "\n",
        "        Crie um grafo conectando esses conceitos. Use rela√ß√µes l√≥gicas.\n",
        "        \"\"\"\n",
        "\n",
        "        # Retry Autom√°tico\n",
        "        for tentativa in range(3):\n",
        "            try:\n",
        "                response = client.models.generate_content(\n",
        "                    model=MODELO_ESTRUTURA,\n",
        "                    contents=prompt,\n",
        "                    config={\n",
        "                        \"response_mime_type\": \"application/json\",\n",
        "                        \"response_schema\": KnowledgeGraph,\n",
        "                    },\n",
        "                )\n",
        "                return response.parsed.model_dump()\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e) or \"RESOURCE_EXHAUSTED\" in str(e):\n",
        "                    print(f\"‚ö†Ô∏è Cota do {MODELO_ESTRUTURA} cheia. Esperando 30s...\")\n",
        "                    time.sleep(30)\n",
        "                else:\n",
        "                    print(f\"‚ùå Erro no Step 2: {e}\")\n",
        "                    return None\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro geral Step 2: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "t3MQyGW3h-8B"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Neo4j"
      ],
      "metadata": {
        "id": "z-31YS5DiBmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step3_salvar_neo4j(dados_dict):\n",
        "    \"\"\"Salva no Neo4j\"\"\"\n",
        "    if 'driver' not in globals() or not driver or not dados_dict: return\n",
        "    print(f\"3. [Neo4j] Salvando dados...\")\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            for node in dados_dict.get('nodes', []):\n",
        "                session.run(\"MERGE (n:Concept {name: $id}) SET n.type = $type\", id=node['id'], type=node.get('type', 'Concept'))\n",
        "            for edge in dados_dict.get('edges', []):\n",
        "                rel = edge['relation'].upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "                # type: ignore\n",
        "                session.run(f\"MATCH (s {{name: $s}}), (t {{name: $t}}) MERGE (s)-[:{rel}]->(t)\", s=edge['source'], t=edge['target'])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro Step 3: {e}\")"
      ],
      "metadata": {
        "id": "lf-63I6biGmv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step4_consultar_neo4j(termos):\n",
        "    \"\"\"L√™ do Neo4j com Valida√ß√£o de Tipo\"\"\"\n",
        "    print(f\"4. [Neo4j] Buscando contexto...\")\n",
        "    if 'driver' not in globals() or not driver: return \"\"\n",
        "    termos_seguros = [str(t) for t in termos if isinstance(t, (str, int))]\n",
        "    if not termos_seguros: return \"\"\n",
        "\n",
        "    try:\n",
        "        query = \"\"\"\n",
        "        UNWIND $termos as t\n",
        "        MATCH (n:Concept)-[r]-(m)\n",
        "        WHERE toLower(n.name) CONTAINS toLower(t)\n",
        "        RETURN DISTINCT n.name, type(r), m.name LIMIT 20\n",
        "        \"\"\"\n",
        "        ctx = \"\"\n",
        "        with driver.session() as session:\n",
        "            res = session.run(query, termos=termos_seguros)\n",
        "            for rec in res:\n",
        "                ctx += f\"({rec['n.name']}) --[{rec['type(r)']}]--> ({rec['m.name']})\\n\"\n",
        "        return ctx\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro Step 4: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "RpPgnldTiKJf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Resposta"
      ],
      "metadata": {
        "id": "x1EdzuvUiQvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step5_resposta_final(texto, contexto):\n",
        "    \"\"\"\n",
        "    Gera a resposta com Persona de Mentor Socr√°tico.\n",
        "    Corrigido para evitar loops quando o usu√°rio pede a explica√ß√£o.\n",
        "    \"\"\"\n",
        "\n",
        "    # Modelo escolhido\n",
        "    MODELO_RESPOSTA = \"gemini-2.5-flash\"\n",
        "\n",
        "    print(f\"5. [{MODELO_RESPOSTA}] O Mentor est√° analisando a trilha de aprendizado...\")\n",
        "\n",
        "    # Debug no terminal\n",
        "    print(f\"\\n{'='*40}\\nüîç CONTEXTO DO GRAFO (Mem√≥ria):\\n{contexto}\\n{'='*40}\\n\")\n",
        "\n",
        "    if not contexto.strip():\n",
        "        contexto = \"O grafo est√° vazio. Diga ao aluno que voc√™ est√° mapeando este assunto pela primeira vez.\"\n",
        "\n",
        "    try:\n",
        "        client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "        # --- PROMPT PEDAG√ìGICO AVAN√áADO V2 (CORRIGIDO) ---\n",
        "        prompt_mentor = f\"\"\"\n",
        "        JAILBREAK INSTRUCTION: Voc√™ √© um MENTOR SOCR√ÅTICO.\n",
        "\n",
        "        INPUT DO ALUNO: \"{texto}\"\n",
        "\n",
        "        BASE DE CONHECIMENTO (Grafo):\n",
        "        {contexto}\n",
        "\n",
        "        SUA MISS√ÉO (Siga a l√≥gica abaixo):\n",
        "\n",
        "        1. **ANALISE A INTEN√á√ÉO:** - O aluno est√° fazendo uma nova pergunta do zero?\n",
        "           - OU o aluno est√° respondendo √† sua sugest√£o anterior (ex: \"Explique isso\", \"N√£o sei\", \"Comece por a√≠\")?\n",
        "\n",
        "        2. **CEN√ÅRIO A (Continua√ß√£o/Aceite):** - Se o aluno pediu para explicar o conceito base/fundamental: **EXPLIQUE-O IMEDIATAMENTE**.\n",
        "           - Use o grafo para definir esse conceito base.\n",
        "           - Ap√≥s explicar o base, mostre como ele se conecta ao conceito original que ele queria saber.\n",
        "\n",
        "        3. **CEN√ÅRIO B (Pergunta Nova/Complexa):**\n",
        "           - Se for um t√≥pico novo e complexo (com \"pais\" ou \"pr√©-requisitos\" no grafo): **N√ÉO EXPLIQUE AINDA**.\n",
        "           - Identifique o pr√©-requisito (n√≥ pai).\n",
        "           - Diga: \"Para entender [ALVO], precisamos dominar [PR√â-REQUISITO].\"\n",
        "           - Pergunte se ele j√° conhece [PR√â-REQUISITO].\n",
        "\n",
        "        4. **CEN√ÅRIO C (Simples):**\n",
        "           - Se n√£o houver depend√™ncias complexas, explique didaticamente.\n",
        "\n",
        "        TOM DE VOZ:\n",
        "        - Did√°tico, paciente e estruturado.\n",
        "        - N√£o repita a pergunta de sondagem se o aluno j√° pediu a explica√ß√£o.\n",
        "        \"\"\"\n",
        "\n",
        "        # Retry L√≥gico\n",
        "        for tentativa in range(3):\n",
        "            try:\n",
        "                response = client.models.generate_content(\n",
        "                    model=MODELO_RESPOSTA,\n",
        "                    contents=prompt_mentor\n",
        "                )\n",
        "                return response.text\n",
        "\n",
        "            except Exception as e:\n",
        "                erro_str = str(e)\n",
        "                if \"429\" in erro_str or \"RESOURCE_EXHAUSTED\" in erro_str:\n",
        "                    print(f\"‚ö†Ô∏è Cota cheia no Step 5 (Tentativa {tentativa+1}). Esperando 30s...\")\n",
        "                    time.sleep(30)\n",
        "                else:\n",
        "                    return f\"Erro na IA: {e}\"\n",
        "\n",
        "        return \"O Mentor est√° indispon√≠vel (Cota). Tente em 1 min.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Erro t√©cnico no Mentor: {e}\""
      ],
      "metadata": {
        "id": "m4-ufaBGiTCT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Interface Gradio (UI)"
      ],
      "metadata": {
        "id": "oaiTQHl5tBCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. INTERFACE GRADIO (MEM√ìRIA CORRIGIDA) ---\n",
        "\n",
        "def pipeline_completo(message, history):\n",
        "    # --- 1. MEM√ìRIA DE CONTEXTO ---\n",
        "    texto_para_analise = message\n",
        "\n",
        "    if history:\n",
        "        # Pega a √∫ltima intera√ß√£o (User, AI)\n",
        "        ultima_interacao = history[-1]\n",
        "        ultima_pergunta_user = ultima_interacao[0]\n",
        "        ultima_resposta_ia = ultima_interacao[1]\n",
        "\n",
        "        # ESTRAT√âGIA DE CONTINUIDADE:\n",
        "        # Em vez de pegar os √∫ltimos caracteres, pegamos um resumo focado.\n",
        "        # Se a resposta anterior for muito longa, pegamos o in√≠cio (onde geralmente est√° a defini√ß√£o) e o fim (pergunta socr√°tica).\n",
        "        if len(ultima_resposta_ia) > 300:\n",
        "            resumo_ia = ultima_resposta_ia[:150] + \" ... \" + ultima_resposta_ia[-150:]\n",
        "        else:\n",
        "            resumo_ia = ultima_resposta_ia\n",
        "\n",
        "        # Monta um prompt claro para o LiquidAI entender a refer√™ncia \"disso/daquilo\"\n",
        "        texto_para_analise = f\"\"\"\n",
        "        Hist√≥rico Recente:\n",
        "        Usu√°rio: {ultima_pergunta_user}\n",
        "        Mentor: {resumo_ia}\n",
        "\n",
        "        Nova Pergunta do Usu√°rio (resolva refer√™ncias como 'isso' ou 'ele'): {message}\n",
        "        \"\"\"\n",
        "\n",
        "    # --- 2. EXTRA√á√ÉO ---\n",
        "    termos = step1_extrair_termos(texto_para_analise)\n",
        "    yield f\"üß† [LiquidAI] Analisando: `{message}`\\n(Contexto detectado: {termos})...\"\n",
        "\n",
        "    # --- 3. VERIFICA√á√ÉO INICIAL ---\n",
        "    contexto = step4_consultar_neo4j(termos)\n",
        "\n",
        "    # --- 4. APRENDIZADO (Se necess√°rio) ---\n",
        "    if not contexto:\n",
        "        # Verifica se realmente precisa aprender ou se foi erro de extra√ß√£o\n",
        "        if len(termos) > 0 and len(termos[0]) > 2:\n",
        "            yield f\"üß† [LiquidAI] T√≥pico `{termos}` novo.\\nüï∏Ô∏è Consultando Gemini para criar o Grafo...\"\n",
        "\n",
        "            dados_grafo = step2_gerar_grafo(termos, message) # Usa a mensagem original aqui para o Gemini entender a d√∫vida real\n",
        "\n",
        "            if dados_grafo:\n",
        "                step3_salvar_neo4j(dados_grafo)\n",
        "                contexto = step4_consultar_neo4j(termos)\n",
        "                yield f\"üß† [LiquidAI] Grafo criado! Gerando aula...\"\n",
        "            else:\n",
        "                contexto = \"Erro: Falha na API ao criar grafo.\"\n",
        "        else:\n",
        "             contexto = \"Aviso: Termos muito vagos para busca no grafo.\"\n",
        "\n",
        "    # --- 5. RESPOSTA FINAL ---\n",
        "    resposta = step5_resposta_final(message, contexto)\n",
        "    yield resposta\n",
        "\n",
        "# Configura a interface com visual LIMPO\n",
        "demo = gr.ChatInterface(\n",
        "    fn=pipeline_completo,\n",
        "    title=\"ü§ñ GrafIA\",\n",
        "    description=\"Mentor Socr√°tico. O sistema aprende (cria grafos) e ensina passo a passo.\",\n",
        "    examples=[\"O que √© RAG?\", \"Explique Docker containers\", \"Como funciona a fotoss√≠ntese?\"],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Interface Pronta! Pode rodar o Launch.\")"
      ],
      "metadata": {
        "id": "BEhxFcHCtHu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557efd0e-66df-460e-c1b6-93e67a23126d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interface Pronta! Pode rodar o Launch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Execu√ß√£o (Launch)"
      ],
      "metadata": {
        "id": "WkhqyLcMi7E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Iniciando servidor Gradio...\")\n",
        "\n",
        "# share=True gera o link p√∫blico tempor√°rio (ex: https://...gradio.live)\n",
        "# debug=True mostra os erros no console do Colab se algo quebrar\n",
        "demo.queue().launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MW3i0c1njCPo",
        "outputId": "7f1d9f3b-49f7-4b2b-c26b-fb64c6b5b3e3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Iniciando servidor Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9bc332db748b015d3c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9bc332db748b015d3c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. [LiquidAI] Analisando: Explique Docker containers...\n",
            "   -> Termos Finais: ['Docker containers']\n",
            "4. [Neo4j] Buscando contexto...\n",
            "2. [Gemini 2.5 Lite] Criando estrutura estruturada...\n",
            "3. [Neo4j] Salvando dados...\n",
            "4. [Neo4j] Buscando contexto...\n",
            "5. [gemini-2.5-flash] O Mentor est√° analisando a trilha de aprendizado...\n",
            "\n",
            "========================================\n",
            "üîç CONTEXTO DO GRAFO (Mem√≥ria):\n",
            "(Docker containers) --[ENABLES]--> (Resource Management)\n",
            "(Docker containers) --[ENABLES]--> (Portability)\n",
            "(Docker containers) --[IS_A_PART_OF]--> (Containerization)\n",
            "(Docker containers) --[PROVIDES]--> (Isolation)\n",
            "(Docker containers) --[MANAGED_BY]--> (Docker Engine)\n",
            "(Docker containers) --[IMPLEMENTS]--> (OS-level virtualization)\n",
            "\n",
            "========================================\n",
            "\n",
            "‚ö†Ô∏è Cota cheia no Step 5 (Tentativa 1). Esperando 30s...\n",
            "‚ö†Ô∏è Cota cheia no Step 5 (Tentativa 2). Esperando 30s...\n",
            "‚ö†Ô∏è Cota cheia no Step 5 (Tentativa 3). Esperando 30s...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9bc332db748b015d3c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}